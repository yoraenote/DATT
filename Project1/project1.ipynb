{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1887554a-cb1b-4505-aa76-055b5cb5659e",
   "metadata": {},
   "source": [
    "# [DATT] Project 1. 선정된 최종모형을 그대로 사용하는 것이 맞을까? 아니면, 전체 데이터 셋에 대해 다시 적합하는 것이 맞을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa9652-f1d1-44d6-a75b-85c4421bc883",
   "metadata": {},
   "source": [
    "- [Background]\n",
    "  - 데이터를 분석하는 과정에서 최종모형과 최적의 Hyper-parameter를 선택하고 난 후에, train 데이터를 적합한 모형을 사용할 것인가? 아니면 train + test 데이터 전체를 합쳐서, 다시 적합한 모형을 사용할 것인가? 에 대한 궁금증이 든 적이 있다. 직관적으로 최적의 Hyper-Parameter는 train 데이터에 의해서 선택된 모수들이기 때문에, 과연 train + test 데이터를 합쳐서 다시 적합하는게 맞을까? 라는 궁금증으로 이어지기도 하였다. 그래서 해당 궁금증을 이번 DATT 첫 번째 project로 다양한 데이터로부터 실험을 통해 검증해 보고자 한다.\n",
    "\n",
    "\n",
    "- [Hypothesis]\n",
    "  - Hypothesis1. 동일한 하이퍼 파라미터로 train sample과 full sample를 돌렸을 때, 정확도는 동일할 것이다.\n",
    "  - Hypothesis2. 동일한 하이퍼 파라미터로 train sample과 full sample를 돌렸을 때, 정확도는 다를 것이다. 만약 다르다면, 어떤 sample을 적용했을 때 더 높은 정확도를 갖는가? ($\\alpha=0.05$)\n",
    "\n",
    "\n",
    "- [Setting]\n",
    "  - Data Case : 총 4개의 데이터 셋으로 검증, 각각의 데이터 셋은 Binary Classification이며, y의 비율은 상이하게 조정 (50%, 35%, 25%, 20%)\n",
    "  - Iteration : 100회 by seed\n",
    "  - Data Setting by each Iteration\n",
    "    - Unknown Data : 전체 데이터의 20%\n",
    "    - Known Data : 전체 데이터의 80%\n",
    "      - Train Data : Known Data의 80%\n",
    "      - Test Data : Known Data의 20%\n",
    "  - Cross-Validation for Optimal Hyper-Parameter : 10-Fold\n",
    "  - Preprocessing\n",
    "    - 연속형 변수 : 그대로 사용\n",
    "    - 범주형 변수 : 속성에 맞게, Label Encoding or One-Hot Encoding\n",
    "  - Model : Random Forest Model\n",
    "    - Number of Trees : [50, 100, 150, 300]\n",
    "    - Max Depth : [None, 10, 20, 30]\n",
    "    - Max Features : [auto, sqrt, log2]\n",
    "  - Fitting & Prediction (Same Hyper-parameter)\n",
    "    - Train Model : Only Train Data -> predict unknown data y from unknown data x\n",
    "    - Full Model : Train + Test Data -> predict unknown data y from unknown data x\n",
    "\n",
    "\n",
    "- [Project Step]\n",
    "  - Step 1. Data Import & Data Preprocessing\n",
    "  - Step 2. Split Unknown & Known Data by seed\n",
    "  - Step 3. Split train & test Data from Known Data by seed\n",
    "  - Step 4. Search for Optimal Hyper-Parameter by F1-Score\n",
    "  - Step 5. Model Fitting & Prediction : Train sample & Full sample (same Hyper-Parameter)\n",
    "  - Step 6. Comparison Prediction Results : Full sample - Train sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baeaf4b-3eed-46f3-ad58-66f28044fb90",
   "metadata": {},
   "source": [
    "## Setting. Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c5bcb3-44f8-45c5-8d4f-5e989611fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Base packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "### Evaluation packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score , recall_score , f1_score\n",
    "\n",
    "### Model packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "### Data packages\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import seaborn as sns\n",
    "\n",
    "### etc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5debd9ad-33d4-45be-8bea-9a892defc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation Function\n",
    "def evaluation(y_test , pred):\n",
    "    accuracy = np.round(accuracy_score(y_test , pred),4)\n",
    "    f1 = np.round(f1_score(y_test,pred),4)\n",
    "    precision = np.round(precision_score(y_test , pred),4)\n",
    "    recall = np.round(recall_score(y_test , pred),4)\n",
    "    \n",
    "    return pd.DataFrame({'Accuracy':[accuracy], 'F1':[f1], 'Precision':[precision], 'Recall':[recall]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc384e5-d18f-4d51-8c9c-e700caf3a92a",
   "metadata": {},
   "source": [
    "## Case 1. Breast Cancer Data\n",
    "\n",
    "- Breast Cancer를 예측하는 데이터셋으로 약 30개의 연속형 변수와 Breast Cancer 여부인 y로 구성되어 있다.\n",
    "- y의 비율은 1이 63% 이고 0은 37%로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65055703-5be4-4d6e-942c-5b7fe46a4bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.627417\n",
       "0    0.372583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 1] Data Import & Data Preprocessing\n",
    "\n",
    "## Data Import\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "## Data Split\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "## Proportion of y\n",
    "pd.DataFrame(y).value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "927024c7-4b10-4463-b05d-9054ef6dfd23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish  Iteration 1\n",
      "Finish  Iteration 2\n",
      "Finish  Iteration 3\n",
      "Finish  Iteration 4\n",
      "Finish  Iteration 5\n",
      "Finish  Iteration 6\n",
      "Finish  Iteration 7\n",
      "Finish  Iteration 8\n",
      "Finish  Iteration 9\n",
      "Finish  Iteration 10\n",
      "Finish  Iteration 11\n",
      "Finish  Iteration 12\n",
      "Finish  Iteration 13\n",
      "Finish  Iteration 14\n",
      "Finish  Iteration 15\n",
      "Finish  Iteration 16\n",
      "Finish  Iteration 17\n",
      "Finish  Iteration 18\n",
      "Finish  Iteration 19\n",
      "Finish  Iteration 20\n",
      "Finish  Iteration 21\n",
      "Finish  Iteration 22\n",
      "Finish  Iteration 23\n",
      "Finish  Iteration 24\n",
      "Finish  Iteration 25\n",
      "Finish  Iteration 26\n",
      "Finish  Iteration 27\n",
      "Finish  Iteration 28\n",
      "Finish  Iteration 29\n",
      "Finish  Iteration 30\n",
      "Finish  Iteration 31\n",
      "Finish  Iteration 32\n",
      "Finish  Iteration 33\n",
      "Finish  Iteration 34\n",
      "Finish  Iteration 35\n",
      "Finish  Iteration 36\n",
      "Finish  Iteration 37\n",
      "Finish  Iteration 38\n",
      "Finish  Iteration 39\n",
      "Finish  Iteration 40\n",
      "Finish  Iteration 41\n",
      "Finish  Iteration 42\n",
      "Finish  Iteration 43\n",
      "Finish  Iteration 44\n",
      "Finish  Iteration 45\n",
      "Finish  Iteration 46\n",
      "Finish  Iteration 47\n",
      "Finish  Iteration 48\n",
      "Finish  Iteration 49\n",
      "Finish  Iteration 50\n",
      "Finish  Iteration 51\n",
      "Finish  Iteration 52\n",
      "Finish  Iteration 53\n",
      "Finish  Iteration 54\n",
      "Finish  Iteration 55\n",
      "Finish  Iteration 56\n",
      "Finish  Iteration 57\n",
      "Finish  Iteration 58\n",
      "Finish  Iteration 59\n",
      "Finish  Iteration 60\n",
      "Finish  Iteration 61\n",
      "Finish  Iteration 62\n",
      "Finish  Iteration 63\n",
      "Finish  Iteration 64\n",
      "Finish  Iteration 65\n",
      "Finish  Iteration 66\n",
      "Finish  Iteration 67\n",
      "Finish  Iteration 68\n",
      "Finish  Iteration 69\n",
      "Finish  Iteration 70\n",
      "Finish  Iteration 71\n",
      "Finish  Iteration 72\n",
      "Finish  Iteration 73\n",
      "Finish  Iteration 74\n",
      "Finish  Iteration 75\n",
      "Finish  Iteration 76\n",
      "Finish  Iteration 77\n",
      "Finish  Iteration 78\n",
      "Finish  Iteration 79\n",
      "Finish  Iteration 80\n",
      "Finish  Iteration 81\n",
      "Finish  Iteration 82\n",
      "Finish  Iteration 83\n",
      "Finish  Iteration 84\n",
      "Finish  Iteration 85\n",
      "Finish  Iteration 86\n",
      "Finish  Iteration 87\n",
      "Finish  Iteration 88\n",
      "Finish  Iteration 89\n",
      "Finish  Iteration 90\n",
      "Finish  Iteration 91\n",
      "Finish  Iteration 92\n",
      "Finish  Iteration 93\n",
      "Finish  Iteration 94\n",
      "Finish  Iteration 95\n",
      "Finish  Iteration 96\n",
      "Finish  Iteration 97\n",
      "Finish  Iteration 98\n",
      "Finish  Iteration 99\n",
      "Finish  Iteration 100\n"
     ]
    }
   ],
   "source": [
    "### Validation for 100-Iteration\n",
    "train_result = pd.DataFrame()\n",
    "full_result = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for set_seed in range(100) :\n",
    "    \n",
    "    ### [Step 2] Split Unknown & Known Data by seed\n",
    "    train_x, unknown_x, train_y, unknown_y = train_test_split(x, y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 3] Split train & test Data from Known Data by seed\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 4] Search for Optimal Hyper-Parameter by F1-Score\n",
    "    \n",
    "    ## Base Model\n",
    "    rf_model = RandomForestClassifier(random_state = set_seed)\n",
    "    \n",
    "    ## Parameter Setting\n",
    "    par = {'n_estimators':[50,100,150,300],\n",
    "           'max_depth':[None,10,20,30],\n",
    "           'max_features':['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    ## Stratified 10-Fold\n",
    "    train_rf = GridSearchCV(rf_model, param_grid = par, scoring = 'f1', cv = 10)\n",
    "    \n",
    "    ### [Step 5] Model Fitting & Prediction : Train sample & Full sample (same Hyper-Parameter)\n",
    "    \n",
    "    ## Train sample Model Fitting\n",
    "    train_rf.fit(x_train, y_train)\n",
    "    \n",
    "    ## Full sample Model Fitting\n",
    "    best_par = train_rf.best_params_\n",
    "    full_rf = RandomForestClassifier(random_state = set_seed,\n",
    "                                     max_depth = best_par['max_depth'],\n",
    "                                     max_features = best_par['max_features'],\n",
    "                                     n_estimators = best_par['n_estimators'])\n",
    "    full_rf.fit(train_x, train_y)\n",
    "    \n",
    "    ## Prediction\n",
    "    train_pred = train_rf.predict(unknown_x)\n",
    "    full_pred = full_rf.predict(unknown_x)  \n",
    "    \n",
    "    train_result = pd.concat([train_result,evaluation(unknown_y, train_pred)],axis=0).reset_index(drop = True)\n",
    "    full_result = pd.concat([full_result,evaluation(unknown_y, full_pred)],axis=0).reset_index(drop = True)\n",
    "    \n",
    "    print(\"Finish Iteration\",i+1)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48653ab0-64eb-40ad-9895-7717523cfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results\n",
    "\n",
    "## Saving Results\n",
    "#train_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data1_train_result.csv\", index = False)\n",
    "#full_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data1_full_result.csv\", index = False)\n",
    "train_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data1_train_result.csv\")\n",
    "full_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data1_full_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3130d032-1de0-4b49-ab3d-bbfbb5a93bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy_full_train</th>\n",
       "      <th>F1_full_train</th>\n",
       "      <th>Precision_full_train</th>\n",
       "      <th>Recall_full_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.002712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.011291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.035100</td>\n",
       "      <td>-0.027300</td>\n",
       "      <td>-0.027400</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy_full_train  F1_full_train  Precision_full_train  \\\n",
       "count           100.000000     100.000000            100.000000   \n",
       "mean              0.003775       0.002999              0.003312   \n",
       "std               0.011312       0.008860              0.012440   \n",
       "min              -0.035100      -0.027300             -0.027400   \n",
       "25%               0.000000       0.000000              0.000000   \n",
       "50%               0.000000       0.000000              0.000000   \n",
       "75%               0.008800       0.007100              0.013150   \n",
       "max               0.035100       0.028000              0.047400   \n",
       "\n",
       "       Recall_full_train  \n",
       "count         100.000000  \n",
       "mean            0.002712  \n",
       "std             0.011291  \n",
       "min            -0.050000  \n",
       "25%             0.000000  \n",
       "50%             0.000000  \n",
       "75%             0.013300  \n",
       "max             0.029900  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 6] Comparison Prediction Results : Full sample - Train sample\n",
    "result1 = pd.DataFrame()\n",
    "result1['Accuracy_full_train'] = full_result['Accuracy'] - train_result['Accuracy']\n",
    "result1['F1_full_train'] = full_result['F1'] - train_result['F1']\n",
    "result1['Precision_full_train'] = full_result['Precision'] - train_result['Precision']\n",
    "result1['Recall_full_train'] = full_result['Recall'] - train_result['Recall']\n",
    "result1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30becd57-e577-4627-9bb6-ed3d8948cf71",
   "metadata": {},
   "source": [
    "## Case 2. Wine Data\n",
    "\n",
    "- Wine의 색깔이 red인지 white인지 구분하는 Binary Classification 문제이며, 11개의 feature로 구성되어 있다.\n",
    "- y의 비율은 0이 75%, 1이 25%로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd7ee98-9667-4da9-a159-3e2ed89f41cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color\n",
       "0        0.753886\n",
       "1        0.246114\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 1] Data Import & Data Preprocessing\n",
    "\n",
    "## Data Import\n",
    "red = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "white = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep=';')\n",
    "red['color'] = 1\n",
    "white['color'] = 0\n",
    "\n",
    "## Data Split\n",
    "data = pd.concat([red,white],axis = 0).reset_index(drop = True)\n",
    "x = data.drop(['color','quality'],axis=1)\n",
    "y = data['color']\n",
    "\n",
    "## Proportion of y\n",
    "pd.DataFrame(y).value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8464f402-7ed6-4a69-a986-130bd5a93d49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Iteration 1\n",
      "Finish Iteration 2\n",
      "Finish Iteration 3\n",
      "Finish Iteration 4\n",
      "Finish Iteration 5\n",
      "Finish Iteration 6\n",
      "Finish Iteration 7\n",
      "Finish Iteration 8\n",
      "Finish Iteration 9\n",
      "Finish Iteration 10\n",
      "Finish Iteration 11\n",
      "Finish Iteration 12\n",
      "Finish Iteration 13\n",
      "Finish Iteration 14\n",
      "Finish Iteration 15\n",
      "Finish Iteration 16\n",
      "Finish Iteration 17\n",
      "Finish Iteration 18\n",
      "Finish Iteration 19\n",
      "Finish Iteration 20\n",
      "Finish Iteration 21\n",
      "Finish Iteration 22\n",
      "Finish Iteration 23\n",
      "Finish Iteration 24\n",
      "Finish Iteration 25\n",
      "Finish Iteration 26\n",
      "Finish Iteration 27\n",
      "Finish Iteration 28\n",
      "Finish Iteration 29\n",
      "Finish Iteration 30\n",
      "Finish Iteration 31\n",
      "Finish Iteration 32\n",
      "Finish Iteration 33\n",
      "Finish Iteration 34\n",
      "Finish Iteration 35\n",
      "Finish Iteration 36\n",
      "Finish Iteration 37\n",
      "Finish Iteration 38\n",
      "Finish Iteration 39\n",
      "Finish Iteration 40\n",
      "Finish Iteration 41\n",
      "Finish Iteration 42\n",
      "Finish Iteration 43\n",
      "Finish Iteration 44\n",
      "Finish Iteration 45\n",
      "Finish Iteration 46\n",
      "Finish Iteration 47\n",
      "Finish Iteration 48\n",
      "Finish Iteration 49\n",
      "Finish Iteration 50\n",
      "Finish Iteration 51\n",
      "Finish Iteration 52\n",
      "Finish Iteration 53\n",
      "Finish Iteration 54\n",
      "Finish Iteration 55\n",
      "Finish Iteration 56\n",
      "Finish Iteration 57\n",
      "Finish Iteration 58\n",
      "Finish Iteration 59\n",
      "Finish Iteration 60\n",
      "Finish Iteration 61\n",
      "Finish Iteration 62\n",
      "Finish Iteration 63\n",
      "Finish Iteration 64\n",
      "Finish Iteration 65\n",
      "Finish Iteration 66\n",
      "Finish Iteration 67\n",
      "Finish Iteration 68\n",
      "Finish Iteration 69\n",
      "Finish Iteration 70\n",
      "Finish Iteration 71\n",
      "Finish Iteration 72\n",
      "Finish Iteration 73\n",
      "Finish Iteration 74\n",
      "Finish Iteration 75\n",
      "Finish Iteration 76\n",
      "Finish Iteration 77\n",
      "Finish Iteration 78\n",
      "Finish Iteration 79\n",
      "Finish Iteration 80\n",
      "Finish Iteration 81\n",
      "Finish Iteration 82\n",
      "Finish Iteration 83\n",
      "Finish Iteration 84\n",
      "Finish Iteration 85\n",
      "Finish Iteration 86\n",
      "Finish Iteration 87\n",
      "Finish Iteration 88\n",
      "Finish Iteration 89\n",
      "Finish Iteration 90\n",
      "Finish Iteration 91\n",
      "Finish Iteration 92\n",
      "Finish Iteration 93\n",
      "Finish Iteration 94\n",
      "Finish Iteration 95\n",
      "Finish Iteration 96\n",
      "Finish Iteration 97\n",
      "Finish Iteration 98\n",
      "Finish Iteration 99\n",
      "Finish Iteration 100\n"
     ]
    }
   ],
   "source": [
    "### Validation for 100-Iteration\n",
    "train_result = pd.DataFrame()\n",
    "full_result = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for set_seed in range(100) :\n",
    "    \n",
    "    ### [Step 2] Split Unknown & Known Data by seed\n",
    "    train_x, unknown_x, train_y, unknown_y = train_test_split(x, y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 3] Split train & test Data from Known Data by seed\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 4] Search for Optimal Hyper-Parameter by F1-Score\n",
    "    \n",
    "    ## Base Model\n",
    "    rf_model = RandomForestClassifier(random_state = set_seed)\n",
    "    \n",
    "    ## Parameter Setting\n",
    "    par = {'n_estimators':[50,100,150,300],\n",
    "           'max_depth':[None,10,20,30],\n",
    "           'max_features':['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    ## Stratified 10-Fold\n",
    "    train_rf = GridSearchCV(rf_model, param_grid = par, scoring = 'f1', cv = 10)\n",
    "    \n",
    "    ### [Step 5] Model Fitting & Prediction : Train sample & Full sample (same Hyper-Parameter)\n",
    "    \n",
    "    ## Train sample Model Fitting\n",
    "    train_rf.fit(x_train, y_train)\n",
    "    \n",
    "    ## Full sample Model Fitting\n",
    "    best_par = train_rf.best_params_\n",
    "    full_rf = RandomForestClassifier(random_state = set_seed,\n",
    "                                     max_depth = best_par['max_depth'],\n",
    "                                     max_features = best_par['max_features'],\n",
    "                                     n_estimators = best_par['n_estimators'])\n",
    "    full_rf.fit(train_x, train_y)\n",
    "    \n",
    "    ## Prediction\n",
    "    train_pred = train_rf.predict(unknown_x)\n",
    "    full_pred = full_rf.predict(unknown_x)  \n",
    "    \n",
    "    train_result = pd.concat([train_result,evaluation(unknown_y, train_pred)],axis=0).reset_index(drop = True)\n",
    "    full_result = pd.concat([full_result,evaluation(unknown_y, full_pred)],axis=0).reset_index(drop = True)\n",
    "    \n",
    "    print(\"Finish Iteration\",i+1)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4d914f-b8a1-4eac-b56d-5b8d5ee6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results\n",
    "\n",
    "## Saving Results\n",
    "#train_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data2_train_result.csv\", index = False)\n",
    "#full_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data2_full_result.csv\", index = False)\n",
    "train_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data2_train_result.csv\")\n",
    "full_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data2_full_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b210f7fc-e33b-4c07-8376-81a8c0b24e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy_full_train</th>\n",
       "      <th>F1_full_train</th>\n",
       "      <th>Precision_full_train</th>\n",
       "      <th>Recall_full_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.001027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.004232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.002300</td>\n",
       "      <td>-0.004900</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000175</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy_full_train  F1_full_train  Precision_full_train  \\\n",
       "count           100.000000     100.000000            100.000000   \n",
       "mean              0.000294       0.000607              0.000158   \n",
       "std               0.001128       0.002343              0.002177   \n",
       "min              -0.002300      -0.004900             -0.006700   \n",
       "25%              -0.000175      -0.000450              0.000000   \n",
       "50%               0.000000       0.000000              0.000000   \n",
       "75%               0.000800       0.001600              0.000100   \n",
       "max               0.003100       0.006600              0.006100   \n",
       "\n",
       "       Recall_full_train  \n",
       "count         100.000000  \n",
       "mean            0.001027  \n",
       "std             0.004232  \n",
       "min            -0.012900  \n",
       "25%             0.000000  \n",
       "50%             0.000000  \n",
       "75%             0.003100  \n",
       "max             0.012900  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 6] Comparison Prediction Results : Full sample - Train sample\n",
    "result2 = pd.DataFrame()\n",
    "result2['Accuracy_full_train'] = full_result['Accuracy'] - train_result['Accuracy']\n",
    "result2['F1_full_train'] = full_result['F1'] - train_result['F1']\n",
    "result2['Precision_full_train'] = full_result['Precision'] - train_result['Precision']\n",
    "result2['Recall_full_train'] = full_result['Recall'] - train_result['Recall']\n",
    "result2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4bc85-9a3f-4dee-9dbf-4123108840a6",
   "metadata": {},
   "source": [
    "## Case 3. Titanic Data\n",
    "\n",
    "- Titanic에서 생존 여부를 예측하는 Binary Classification 문제이며, y의 비율을 21%로 맞추기 위해 인위적으로 데이터를 split하였다.\n",
    "- 사용된 feature들은 총 5개이며, 성별의 경우 문자형으로 되어 있어, one-hot encoding을 진행하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621d182a-e7db-431f-9bfb-2bbf512edba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived\n",
       "0           0.784286\n",
       "1           0.215714\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 1] Data Import & Data Preprocessing\n",
    "\n",
    "## Data Import\n",
    "data = sns.load_dataset('titanic')\n",
    "\n",
    "np.random.seed(1234)\n",
    "idx = data[data.survived == 1].index\n",
    "idx1 = np.random.choice(idx, size = 151,replace = False)\n",
    "data = data[(data.survived == 0) | (data.index.isin(idx1))].reset_index(drop=True)\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "## Missing Value Remove\n",
    "data.isnull().sum()\n",
    "\n",
    "## Encoding\n",
    "data = data[['survived','pclass','sex','sibsp','parch','fare']]\n",
    "data = pd.get_dummies(data, columns=['sex'])\n",
    "\n",
    "## Data Split\n",
    "x = data.drop(['survived'], axis = 1)\n",
    "y = data['survived']\n",
    "\n",
    "## Proportion of y\n",
    "pd.DataFrame(y).value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "60ee91a1-6f59-434e-965b-49b398318b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Iteration 1\n",
      "Finish Iteration 2\n",
      "Finish Iteration 3\n",
      "Finish Iteration 4\n",
      "Finish Iteration 5\n",
      "Finish Iteration 6\n",
      "Finish Iteration 7\n",
      "Finish Iteration 8\n",
      "Finish Iteration 9\n",
      "Finish Iteration 10\n",
      "Finish Iteration 11\n",
      "Finish Iteration 12\n",
      "Finish Iteration 13\n",
      "Finish Iteration 14\n",
      "Finish Iteration 15\n",
      "Finish Iteration 16\n",
      "Finish Iteration 17\n",
      "Finish Iteration 18\n",
      "Finish Iteration 19\n",
      "Finish Iteration 20\n",
      "Finish Iteration 21\n",
      "Finish Iteration 22\n",
      "Finish Iteration 23\n",
      "Finish Iteration 24\n",
      "Finish Iteration 25\n",
      "Finish Iteration 26\n",
      "Finish Iteration 27\n",
      "Finish Iteration 28\n",
      "Finish Iteration 29\n",
      "Finish Iteration 30\n",
      "Finish Iteration 31\n",
      "Finish Iteration 32\n",
      "Finish Iteration 33\n",
      "Finish Iteration 34\n",
      "Finish Iteration 35\n",
      "Finish Iteration 36\n",
      "Finish Iteration 37\n",
      "Finish Iteration 38\n",
      "Finish Iteration 39\n",
      "Finish Iteration 40\n",
      "Finish Iteration 41\n",
      "Finish Iteration 42\n",
      "Finish Iteration 43\n",
      "Finish Iteration 44\n",
      "Finish Iteration 45\n",
      "Finish Iteration 46\n",
      "Finish Iteration 47\n",
      "Finish Iteration 48\n",
      "Finish Iteration 49\n",
      "Finish Iteration 50\n",
      "Finish Iteration 51\n",
      "Finish Iteration 52\n",
      "Finish Iteration 53\n",
      "Finish Iteration 54\n",
      "Finish Iteration 55\n",
      "Finish Iteration 56\n",
      "Finish Iteration 57\n",
      "Finish Iteration 58\n",
      "Finish Iteration 59\n",
      "Finish Iteration 60\n",
      "Finish Iteration 61\n",
      "Finish Iteration 62\n",
      "Finish Iteration 63\n",
      "Finish Iteration 64\n",
      "Finish Iteration 65\n",
      "Finish Iteration 66\n",
      "Finish Iteration 67\n",
      "Finish Iteration 68\n",
      "Finish Iteration 69\n",
      "Finish Iteration 70\n",
      "Finish Iteration 71\n",
      "Finish Iteration 72\n",
      "Finish Iteration 73\n",
      "Finish Iteration 74\n",
      "Finish Iteration 75\n",
      "Finish Iteration 76\n",
      "Finish Iteration 77\n",
      "Finish Iteration 78\n",
      "Finish Iteration 79\n",
      "Finish Iteration 80\n",
      "Finish Iteration 81\n",
      "Finish Iteration 82\n",
      "Finish Iteration 83\n",
      "Finish Iteration 84\n",
      "Finish Iteration 85\n",
      "Finish Iteration 86\n",
      "Finish Iteration 87\n",
      "Finish Iteration 88\n",
      "Finish Iteration 89\n",
      "Finish Iteration 90\n",
      "Finish Iteration 91\n",
      "Finish Iteration 92\n",
      "Finish Iteration 93\n",
      "Finish Iteration 94\n",
      "Finish Iteration 95\n",
      "Finish Iteration 96\n",
      "Finish Iteration 97\n",
      "Finish Iteration 98\n",
      "Finish Iteration 99\n",
      "Finish Iteration 100\n"
     ]
    }
   ],
   "source": [
    "### Validation for 100-Iteration\n",
    "train_result = pd.DataFrame()\n",
    "full_result = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for set_seed in range(100) :\n",
    "    \n",
    "    ### [Step 2] Split Unknown & Known Data by seed\n",
    "    train_x, unknown_x, train_y, unknown_y = train_test_split(x, y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 3] Split train & test Data from Known Data by seed\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 4] Search for Optimal Hyper-Parameter by F1-Score\n",
    "    \n",
    "    ## Base Model\n",
    "    rf_model = RandomForestClassifier(random_state = set_seed)\n",
    "    \n",
    "    ## Parameter Setting\n",
    "    par = {'n_estimators':[50,100,150,300],\n",
    "           'max_depth':[None,10,20,30],\n",
    "           'max_features':['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    ## Stratified 10-Fold\n",
    "    train_rf = GridSearchCV(rf_model, param_grid = par, scoring = 'f1', cv = 10)\n",
    "    \n",
    "    ### [Step 5] Model Fitting & Prediction : Train sample & Full sample (same Hyper-Parameter)\n",
    "    \n",
    "    ## Train sample Model Fitting\n",
    "    train_rf.fit(x_train, y_train)\n",
    "    \n",
    "    ## Full sample Model Fitting\n",
    "    best_par = train_rf.best_params_\n",
    "    full_rf = RandomForestClassifier(random_state = set_seed,\n",
    "                                     max_depth = best_par['max_depth'],\n",
    "                                     max_features = best_par['max_features'],\n",
    "                                     n_estimators = best_par['n_estimators'])\n",
    "    full_rf.fit(train_x, train_y)\n",
    "    \n",
    "    ## Prediction\n",
    "    train_pred = train_rf.predict(unknown_x)\n",
    "    full_pred = full_rf.predict(unknown_x)  \n",
    "    \n",
    "    train_result = pd.concat([train_result,evaluation(unknown_y, train_pred)],axis=0).reset_index(drop = True)\n",
    "    full_result = pd.concat([full_result,evaluation(unknown_y, full_pred)],axis=0).reset_index(drop = True)\n",
    "    \n",
    "    print(\"Finish Iteration\",i+1)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a6e027f-5301-4226-8d83-5720107e8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results\n",
    "\n",
    "## Saving Results\n",
    "#train_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data3_train_result.csv\", index = False)\n",
    "#full_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data3_full_result.csv\", index = False)\n",
    "\n",
    "train_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data3_train_result.csv\")\n",
    "full_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data3_full_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbdadf59-d8fa-4604-92ca-32c7f9270a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy_full_train</th>\n",
       "      <th>F1_full_train</th>\n",
       "      <th>Precision_full_train</th>\n",
       "      <th>Recall_full_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.007787</td>\n",
       "      <td>0.016564</td>\n",
       "      <td>0.026847</td>\n",
       "      <td>0.009862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.015566</td>\n",
       "      <td>0.040892</td>\n",
       "      <td>0.059350</td>\n",
       "      <td>0.054871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.035700</td>\n",
       "      <td>-0.077500</td>\n",
       "      <td>-0.146500</td>\n",
       "      <td>-0.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009025</td>\n",
       "      <td>-0.006425</td>\n",
       "      <td>-0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.042250</td>\n",
       "      <td>0.053250</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy_full_train  F1_full_train  Precision_full_train  \\\n",
       "count           100.000000     100.000000            100.000000   \n",
       "mean              0.007787       0.016564              0.026847   \n",
       "std               0.015566       0.040892              0.059350   \n",
       "min              -0.035700      -0.077500             -0.146500   \n",
       "25%               0.000000      -0.009025             -0.006425   \n",
       "50%               0.007100       0.011500              0.026050   \n",
       "75%               0.014300       0.042250              0.053250   \n",
       "max               0.050000       0.130400              0.227300   \n",
       "\n",
       "       Recall_full_train  \n",
       "count         100.000000  \n",
       "mean            0.009862  \n",
       "std             0.054871  \n",
       "min            -0.137900  \n",
       "25%            -0.031200  \n",
       "50%             0.000000  \n",
       "75%             0.054800  \n",
       "max             0.125000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 6] Comparison Prediction Results : Full sample - Train sample\n",
    "result3 = pd.DataFrame()\n",
    "result3['Accuracy_full_train'] = full_result['Accuracy'] - train_result['Accuracy']\n",
    "result3['F1_full_train'] = full_result['F1'] - train_result['F1']\n",
    "result3['Precision_full_train'] = full_result['Precision'] - train_result['Precision']\n",
    "result3['Recall_full_train'] = full_result['Recall'] - train_result['Recall']\n",
    "result3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835dbda-f155-42ed-9d4a-25c265820509",
   "metadata": {},
   "source": [
    "## Case 4. Penguins Data\n",
    "\n",
    "- penguin의 성별을 예측하는 Binary Classification 문제이며, 사용된 feature는 총 6개이다.\n",
    "- y의 비율은 각각 50%이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aff2045-f03f-4cac-ace0-5e37e526bf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "1.0    0.504505\n",
       "0.0    0.495495\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 1] Data Import & Data Preprocessing\n",
    "\n",
    "## Data Import\n",
    "data = sns.load_dataset('penguins')\n",
    "\n",
    "## Data Encoding\n",
    "data = pd.get_dummies(data, columns=['species','island'])\n",
    "data['sex'].replace({'Female': 0, 'Male': 1}, inplace=True)\n",
    "\n",
    "## MIssing Value drop\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "## Data Split\n",
    "x = data.drop(['sex'], axis = 1)\n",
    "y = data['sex']\n",
    "\n",
    "## Proportion of y\n",
    "pd.DataFrame(y).value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc6fb343-2f28-45ea-afd0-81af80a13f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Iteration 1\n",
      "Finish Iteration 2\n",
      "Finish Iteration 3\n",
      "Finish Iteration 4\n",
      "Finish Iteration 5\n",
      "Finish Iteration 6\n",
      "Finish Iteration 7\n",
      "Finish Iteration 8\n",
      "Finish Iteration 9\n",
      "Finish Iteration 10\n",
      "Finish Iteration 11\n",
      "Finish Iteration 12\n",
      "Finish Iteration 13\n",
      "Finish Iteration 14\n",
      "Finish Iteration 15\n",
      "Finish Iteration 16\n",
      "Finish Iteration 17\n",
      "Finish Iteration 18\n",
      "Finish Iteration 19\n",
      "Finish Iteration 20\n",
      "Finish Iteration 21\n",
      "Finish Iteration 22\n",
      "Finish Iteration 23\n",
      "Finish Iteration 24\n",
      "Finish Iteration 25\n",
      "Finish Iteration 26\n",
      "Finish Iteration 27\n",
      "Finish Iteration 28\n",
      "Finish Iteration 29\n",
      "Finish Iteration 30\n",
      "Finish Iteration 31\n",
      "Finish Iteration 32\n",
      "Finish Iteration 33\n",
      "Finish Iteration 34\n",
      "Finish Iteration 35\n",
      "Finish Iteration 36\n",
      "Finish Iteration 37\n",
      "Finish Iteration 38\n",
      "Finish Iteration 39\n",
      "Finish Iteration 40\n",
      "Finish Iteration 41\n",
      "Finish Iteration 42\n",
      "Finish Iteration 43\n",
      "Finish Iteration 44\n",
      "Finish Iteration 45\n",
      "Finish Iteration 46\n",
      "Finish Iteration 47\n",
      "Finish Iteration 48\n",
      "Finish Iteration 49\n",
      "Finish Iteration 50\n",
      "Finish Iteration 51\n",
      "Finish Iteration 52\n",
      "Finish Iteration 53\n",
      "Finish Iteration 54\n",
      "Finish Iteration 55\n",
      "Finish Iteration 56\n",
      "Finish Iteration 57\n",
      "Finish Iteration 58\n",
      "Finish Iteration 59\n",
      "Finish Iteration 60\n",
      "Finish Iteration 61\n",
      "Finish Iteration 62\n",
      "Finish Iteration 63\n",
      "Finish Iteration 64\n",
      "Finish Iteration 65\n",
      "Finish Iteration 66\n",
      "Finish Iteration 67\n",
      "Finish Iteration 68\n",
      "Finish Iteration 69\n",
      "Finish Iteration 70\n",
      "Finish Iteration 71\n",
      "Finish Iteration 72\n",
      "Finish Iteration 73\n",
      "Finish Iteration 74\n",
      "Finish Iteration 75\n",
      "Finish Iteration 76\n",
      "Finish Iteration 77\n",
      "Finish Iteration 78\n",
      "Finish Iteration 79\n",
      "Finish Iteration 80\n",
      "Finish Iteration 81\n",
      "Finish Iteration 82\n",
      "Finish Iteration 83\n",
      "Finish Iteration 84\n",
      "Finish Iteration 85\n",
      "Finish Iteration 86\n",
      "Finish Iteration 87\n",
      "Finish Iteration 88\n",
      "Finish Iteration 89\n",
      "Finish Iteration 90\n",
      "Finish Iteration 91\n",
      "Finish Iteration 92\n",
      "Finish Iteration 93\n",
      "Finish Iteration 94\n",
      "Finish Iteration 95\n",
      "Finish Iteration 96\n",
      "Finish Iteration 97\n",
      "Finish Iteration 98\n",
      "Finish Iteration 99\n",
      "Finish Iteration 100\n"
     ]
    }
   ],
   "source": [
    "### Validation for 100-Iteration\n",
    "train_result = pd.DataFrame()\n",
    "full_result = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for set_seed in range(100) :\n",
    "    \n",
    "    ### [Step 2] Split Unknown & Known Data by seed\n",
    "    train_x, unknown_x, train_y, unknown_y = train_test_split(x, y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 3] Split train & test Data from Known Data by seed\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state = set_seed)\n",
    "    \n",
    "    ### [Step 4] Search for Optimal Hyper-Parameter by F1-Score\n",
    "    \n",
    "    ## Base Model\n",
    "    rf_model = RandomForestClassifier(random_state = set_seed)\n",
    "    \n",
    "    ## Parameter Setting\n",
    "    par = {'n_estimators':[50,100,150,300],\n",
    "           'max_depth':[None,10,20,30],\n",
    "           'max_features':['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    ## Stratified 10-Fold\n",
    "    train_rf = GridSearchCV(rf_model, param_grid = par, scoring = 'f1', cv = 10)\n",
    "    \n",
    "    ### [Step 5] Model Fitting & Prediction : Train sample & Full sample (same Hyper-Parameter)\n",
    "    \n",
    "    ## Train sample Model Fitting\n",
    "    train_rf.fit(x_train, y_train)\n",
    "    \n",
    "    ## Full sample Model Fitting\n",
    "    best_par = train_rf.best_params_\n",
    "    full_rf = RandomForestClassifier(random_state = set_seed,\n",
    "                                     max_depth = best_par['max_depth'],\n",
    "                                     max_features = best_par['max_features'],\n",
    "                                     n_estimators = best_par['n_estimators'])\n",
    "    full_rf.fit(train_x, train_y)\n",
    "    \n",
    "    ## Prediction\n",
    "    train_pred = train_rf.predict(unknown_x)\n",
    "    full_pred = full_rf.predict(unknown_x)  \n",
    "    \n",
    "    train_result = pd.concat([train_result,evaluation(unknown_y, train_pred)],axis=0).reset_index(drop = True)\n",
    "    full_result = pd.concat([full_result,evaluation(unknown_y, full_pred)],axis=0).reset_index(drop = True)\n",
    "    \n",
    "    print(\"Finish Iteration\",i+1)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10cf11a6-2572-4945-a377-f4c2a269df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results\n",
    "\n",
    "## Saving Results\n",
    "#train_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data4_train_result.csv\", index = False)\n",
    "#full_result.to_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data4_full_result.csv\", index = False)\n",
    "train_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data4_train_result.csv\")\n",
    "full_result = pd.read_csv(\"C:/Users/kyt28/OneDrive/바탕 화면/Data이모저모/Project1/data4_full_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86087f6d-6379-4742-82d6-c1c18f2f1070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy_full_train</th>\n",
       "      <th>F1_full_train</th>\n",
       "      <th>Precision_full_train</th>\n",
       "      <th>Recall_full_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.004332</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.007393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022380</td>\n",
       "      <td>0.024080</td>\n",
       "      <td>0.031474</td>\n",
       "      <td>0.036971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.044800</td>\n",
       "      <td>-0.052500</td>\n",
       "      <td>-0.087900</td>\n",
       "      <td>-0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.014900</td>\n",
       "      <td>-0.013625</td>\n",
       "      <td>-0.007725</td>\n",
       "      <td>-0.025775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.026275</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.090900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy_full_train  F1_full_train  Precision_full_train  \\\n",
       "count           100.000000     100.000000            100.000000   \n",
       "mean              0.004332       0.004968              0.001868   \n",
       "std               0.022380       0.024080              0.031474   \n",
       "min              -0.044800      -0.052500             -0.087900   \n",
       "25%              -0.014900      -0.013625             -0.007725   \n",
       "50%               0.000000       0.000000              0.000000   \n",
       "75%               0.029800       0.026275              0.024725   \n",
       "max               0.044800       0.051300              0.072900   \n",
       "\n",
       "       Recall_full_train  \n",
       "count         100.000000  \n",
       "mean            0.007393  \n",
       "std             0.036971  \n",
       "min            -0.066700  \n",
       "25%            -0.025775  \n",
       "50%             0.000000  \n",
       "75%             0.030300  \n",
       "max             0.090900  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### [Step 6] Comparison Prediction Results : Full sample - Train sample\n",
    "result4 = pd.DataFrame()\n",
    "result4['Accuracy_full_train'] = full_result['Accuracy'] - train_result['Accuracy']\n",
    "result4['F1_full_train'] = full_result['F1'] - train_result['F1']\n",
    "result4['Precision_full_train'] = full_result['Precision'] - train_result['Precision']\n",
    "result4['Recall_full_train'] = full_result['Recall'] - train_result['Recall']\n",
    "result4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad9b77-91c8-47f9-9140-abe84db9ec9b",
   "metadata": {},
   "source": [
    "## Project 1. Summary\n",
    "\n",
    "- 먼저, 100번의 Iteration에 따라 Full sample과 Train sample를 적합한 모형을 구축하고 unknown data에 대한 평가를 진행하였다.\n",
    "- 그렇게 해당 모형의 평가를 4가지 평가지표 Accuracy, F1-Score, Precision, Recall를 구하였다.\n",
    "- 이후, 각각의 Case에서 모든 Metric에 대해 Full sample에서 Train sample를 뺀 차이 값을 구하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee649d-bf57-44cf-a2bf-c04aca3d5987",
   "metadata": {},
   "source": [
    "### Summary 1. Difference Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "38f47448-3638-407c-930e-58c4eba360aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case 1. P(Y=1)=37%</th>\n",
       "      <th>Case 2. P(Y=1)=25%</th>\n",
       "      <th>Case 3. P(Y=1)=21%</th>\n",
       "      <th>Case 4. P(Y=1)=50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.003775 (0.011312)</td>\n",
       "      <td>0.000294 (0.001128)</td>\n",
       "      <td>0.007787 (0.015566)</td>\n",
       "      <td>0.004332 (0.02238)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.002999 (0.00886)</td>\n",
       "      <td>0.000607 (0.002343)</td>\n",
       "      <td>0.016564 (0.040892)</td>\n",
       "      <td>0.004968 (0.02408)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.003312 (0.01244)</td>\n",
       "      <td>0.000158 (0.002177)</td>\n",
       "      <td>0.026847 (0.05935)</td>\n",
       "      <td>0.001868 (0.031474)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.002712 (0.011291)</td>\n",
       "      <td>0.001027 (0.004232)</td>\n",
       "      <td>0.009862 (0.054871)</td>\n",
       "      <td>0.007393 (0.036971)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Case 1. P(Y=1)=37%   Case 2. P(Y=1)=25%   Case 3. P(Y=1)=21%  \\\n",
       "Accuracy   0.003775 (0.011312)  0.000294 (0.001128)  0.007787 (0.015566)   \n",
       "F1-Score    0.002999 (0.00886)  0.000607 (0.002343)  0.016564 (0.040892)   \n",
       "Precision   0.003312 (0.01244)  0.000158 (0.002177)   0.026847 (0.05935)   \n",
       "Recall     0.002712 (0.011291)  0.001027 (0.004232)  0.009862 (0.054871)   \n",
       "\n",
       "            Case 4. P(Y=1)=50%  \n",
       "Accuracy    0.004332 (0.02238)  \n",
       "F1-Score    0.004968 (0.02408)  \n",
       "Precision  0.001868 (0.031474)  \n",
       "Recall     0.007393 (0.036971)  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Acc = pd.DataFrame()\n",
    "F1 = pd.DataFrame()\n",
    "Precision = pd.DataFrame()\n",
    "Recall = pd.DataFrame()\n",
    "result_1 = pd.DataFrame()\n",
    "\n",
    "j = 0\n",
    "for metric in ['Acc','F1','Precision','Recall'] : \n",
    "    for i in range(1,5) :\n",
    "        globals()['r{}'.format(i)] = globals()['result{}'.format(i)].describe().transpose()[['mean','std']]\n",
    "        globals()['{}'.format(metric)] = pd.concat([globals()['{}'.format(metric)],globals()['r{}'.format(i)].iloc[j,:]],axis=1)\n",
    "    j += 1\n",
    "    globals()['{}'.format(metric)].columns = ['Case 1. P(Y=1)=37%','Case 2. P(Y=1)=25%','Case 3. P(Y=1)=21%','Case 4. P(Y=1)=50%']\n",
    "    \n",
    "    d = globals()['{}'.format(metric)].transpose()\n",
    "    d['mean'] = d.apply(lambda row: f\"{np.round(row['mean'],6)} ({np.round(row['std'],6)})\", axis=1)\n",
    "    d = d.drop(columns='std').transpose()\n",
    "    \n",
    "    result_1 = pd.concat([result_1,d],axis=0)\n",
    "    \n",
    "result_1.index = ['Accuracy','F1-Score','Precision','Recall']\n",
    "result_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b26304-09d5-405e-a254-eace479ef0be",
   "metadata": {},
   "source": [
    "- 해당 결과에서 각 cell은 평균 (표준편차)를 의미한다.\n",
    "- 모든 Case에서 4개의 Metric은 모두 Full Sample을 적합한 모형이 Train Sample을 적합한 모형보다 미세하지만 높게 나타났다.\n",
    "- 하지만, 모든 Case에 대해 4개의 Metric의 Confidence Interval을 보면 모두 0을 포함하고 있으므로, 각 유의수준 5% 하에서 통계적 차이는 존재하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3c0e9-e6c4-4cb3-bfb8-f31f1cd87ae9",
   "metadata": {},
   "source": [
    "### Summary 2. Positive Proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0ea86a33-ed13-4ffb-9683-d648541255dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>Metric</th>\n",
       "      <th>P(Full &gt; Train)</th>\n",
       "      <th>P(Full = Train)</th>\n",
       "      <th>P(Full &lt; Train)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case 1. P(Y=1)=37%</td>\n",
       "      <td>Accuracy_full_train</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case 1. P(Y=1)=37%</td>\n",
       "      <td>F1_full_train</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case 1. P(Y=1)=37%</td>\n",
       "      <td>Precision_full_train</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case 1. P(Y=1)=37%</td>\n",
       "      <td>Recall_full_train</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case 2. P(Y=1)=25%</td>\n",
       "      <td>Accuracy_full_train</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Case 2. P(Y=1)=25%</td>\n",
       "      <td>F1_full_train</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Case 2. P(Y=1)=25%</td>\n",
       "      <td>Precision_full_train</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Case 2. P(Y=1)=25%</td>\n",
       "      <td>Recall_full_train</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Case 3. P(Y=1)=21%</td>\n",
       "      <td>Accuracy_full_train</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Case 3. P(Y=1)=21%</td>\n",
       "      <td>F1_full_train</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Case 3. P(Y=1)=21%</td>\n",
       "      <td>Precision_full_train</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Case 3. P(Y=1)=21%</td>\n",
       "      <td>Recall_full_train</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Case 4. P(Y=1)=50%</td>\n",
       "      <td>Accuracy_full_train</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Case 4. P(Y=1)=50%</td>\n",
       "      <td>F1_full_train</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Case 4. P(Y=1)=50%</td>\n",
       "      <td>Precision_full_train</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Case 4. P(Y=1)=50%</td>\n",
       "      <td>Recall_full_train</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Case                Metric  P(Full > Train)  \\\n",
       "0   Case 1. P(Y=1)=37%   Accuracy_full_train             0.42   \n",
       "1   Case 1. P(Y=1)=37%         F1_full_train             0.45   \n",
       "2   Case 1. P(Y=1)=37%  Precision_full_train             0.45   \n",
       "3   Case 1. P(Y=1)=37%     Recall_full_train             0.28   \n",
       "4   Case 2. P(Y=1)=25%   Accuracy_full_train             0.42   \n",
       "5   Case 2. P(Y=1)=25%         F1_full_train             0.44   \n",
       "6   Case 2. P(Y=1)=25%  Precision_full_train             0.27   \n",
       "7   Case 2. P(Y=1)=25%     Recall_full_train             0.39   \n",
       "8   Case 3. P(Y=1)=21%   Accuracy_full_train             0.63   \n",
       "9   Case 3. P(Y=1)=21%         F1_full_train             0.63   \n",
       "10  Case 3. P(Y=1)=21%  Precision_full_train             0.66   \n",
       "11  Case 3. P(Y=1)=21%     Recall_full_train             0.41   \n",
       "12  Case 4. P(Y=1)=50%   Accuracy_full_train             0.42   \n",
       "13  Case 4. P(Y=1)=50%         F1_full_train             0.47   \n",
       "14  Case 4. P(Y=1)=50%  Precision_full_train             0.44   \n",
       "15  Case 4. P(Y=1)=50%     Recall_full_train             0.37   \n",
       "\n",
       "    P(Full = Train)  P(Full < Train)  \n",
       "0              0.43             0.15  \n",
       "1              0.36             0.19  \n",
       "2              0.36             0.19  \n",
       "3              0.62             0.10  \n",
       "4              0.33             0.25  \n",
       "5              0.29             0.27  \n",
       "6              0.57             0.16  \n",
       "7              0.37             0.24  \n",
       "8              0.16             0.21  \n",
       "9              0.08             0.29  \n",
       "10             0.06             0.28  \n",
       "11             0.28             0.31  \n",
       "12             0.25             0.33  \n",
       "13             0.13             0.40  \n",
       "14             0.15             0.41  \n",
       "15             0.33             0.30  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2 = pd.DataFrame()\n",
    "for i in range(1,5) :\n",
    "    res = pd.concat([(globals()['result{}'.format(i)] > 0).sum()/100,(globals()['result{}'.format(i)] == 0).sum()/100,(globals()['result{}'.format(i)] < 0).sum()/100], axis=1)\n",
    "    result_2 = pd.concat([result_2,res],axis=0)\n",
    "result_2 = result_2.reset_index().reset_index()\n",
    "result_2.columns = ['Case','Metric','P(Full > Train)','P(Full = Train)','P(Full < Train)']\n",
    "result_2['Case'] = np.repeat([\"Case 1. P(Y=1)=37%\",\"Case 2. P(Y=1)=25%\",\"Case 3. P(Y=1)=21%\",\"Case 4. P(Y=1)=50%\"], 4)\n",
    "result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91235940-399e-4fef-b475-fc5c87ddc1ec",
   "metadata": {},
   "source": [
    "- 모든 Case 별로 각 Iteration별로 Full sample과 Train sample의 차이에 대해 비율을 분석한 결과, 모든 Metric에서 Full sample를 통해 적합한 모형의 성능이 더 좋게 나타났다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb5b64-7d97-4f0e-8876-5715088186c0",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- Train을 통해 최적의 Hyper-Parameter를 선택하고, 모든 sample(Train + Test)을 통해 다시 적합한 모형이 Train 데이터만으로 예측한 결과보다, 모든 Case에서 4가지의 Metric 관점에서 약간 더 높게 나타났다. (Random Forest)\n",
    "- 하지만, 이러한 차이는 통계적 관점에서 보았을 때, 통계적으로 차이가 존재한다고 볼 수 없다. (유의수준 5% 하)\n",
    "\n",
    "### Opinion\n",
    "- 위와 같은 검증을 통해 궁금증을 풀어볼 수 있는 기회가 있었다. 예상대로 Full sample을 모두 넣고 다시 적합한 모형의 모든 metric이 train sample보다 높았다. 하지만, 이러한 차이는 통계적 관점에서 유의하지 않기 때문에, 데이터 분석 과정에서 최종모형을 선정하고 재적합할지 말지에 대한 고찰이 필요한 것 같다.\n",
    "- 보통적으로 적당한 양의 데이터를 분석하는 상황에서는, Full sample을 통해 조금이라도 기존 모형보다 높은 성능의 모형을 구축하는 것이 적절한 것 같다. 하지만, 많은 양의 데이터를 분석하는 데에는 많은 시간과 비용이 들기 때문에, 다시 sample을 적합할 필요 없이 Train sample로 구축된 모형을 그대로 사용하는 것이 올바른 선택인 것 같다. (일반적으로 고도화된 모형일수록 시간과 비용이 많이 들기 때문에, 기업의 관점에서는 모든 것이 비용이라고 생각한다.)\n",
    "- 그래서 이러한 연구를 통해 데이터 분석 상황에 맞게, 시간과 비용을 고려하여 효율적으로 최종 모형을 Full sample에 대해 다시 적합할지, 아니면 Train sample을 그대로 사용할지에 대한 판단을 내릴 수 있게 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e796747-356d-4adb-ab18-132be9b54406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
